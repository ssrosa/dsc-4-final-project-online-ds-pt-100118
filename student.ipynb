{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Steven Rosa\n",
    "* Student pace: part time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Jeff Herman\n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work flows\n",
    "#### Parrish\n",
    "https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469\n",
    "\n",
    "#### Graph convolutional network (\"semi-supervised\")\n",
    "https://towardsdatascience.com/text-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gutenberg.org/ebooks/4217'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from numpy import dot #parrish\n",
    "from numpy.linalg import norm #parrish\n",
    "import en_core_web_sm #parrish\n",
    "import spacy #parrish \n",
    "from __future__ import unicode_literals #parrish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords: \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "import string\n",
    "\n",
    "# Now we have a list that includes all english stopwords, as well as all punctuation\n",
    "stopwords_list += list(string.punctuation)\n",
    "\n",
    "#https://learn.co/tracks/data-science-career-v1-1/module-4-advanced-machine-learning-deep-learning/section-37-foundations-of-natural-language-processing-nlp/feature-engineering-for-text-data#\n",
    "\n",
    "#&c &c &c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parrish workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chapter1.txt', 'r', encoding = 'utf-8') as f1, \\\n",
    "    open('chapter2.txt', 'r', encoding = 'utf-8') as f2, \\\n",
    "    open('chapter3.txt', 'r', encoding = 'utf-8') as f3, \\\n",
    "    open('chapter4.txt', 'r', encoding = 'utf-8') as f4, \\\n",
    "    open('chapter5.txt', 'r', encoding = 'utf-8') as f5:\n",
    "    ch1 = nlp(f1.read())\n",
    "    ch2 = nlp(f2.read())\n",
    "    ch3 = nlp(f3.read())\n",
    "    ch4 = nlp(f4.read())\n",
    "    ch5 = nlp(f5.read())\n",
    "\n",
    "tokens1 = list(set([w.text for w in ch1 if w.is_alpha]))\n",
    "tokens2 = list(set([w.text for w in ch2 if w.is_alpha]))\n",
    "tokens3 = list(set([w.text for w in ch3 if w.is_alpha]))\n",
    "tokens4 = list(set([w.text for w in ch4 if w.is_alpha]))\n",
    "tokens5 = list(set([w.text for w in ch5 if w.is_alpha]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Parrish\n",
    "nlp.vocab['cheese'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions from Parrish\n",
    "\n",
    "\n",
    "def subtractv(coord1, coord2):\n",
    "    return [c1 - c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "def addv(coord1, coord2):\n",
    "    return [c1 + c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "def meanv(coords):\n",
    "    # assumes every item in coords has same length as item 0\n",
    "    sumv = [0] * len(coords[0])\n",
    "    for item in coords:\n",
    "        for i in range(len(item)):\n",
    "            sumv[i] += item[i]\n",
    "    mean = [0] * len(sumv)\n",
    "    for i in range(len(sumv)):\n",
    "        mean[i] = float(sumv[i]) / len(coords)\n",
    "    return mean\n",
    "\n",
    "\n",
    "#Convenience function\n",
    "def vec(s):\n",
    "    return nlp.vocab[s].vector\n",
    "\n",
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def spacy_closest(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine(vec_to_check, vec(x)),\n",
    "                  reverse=True)[:n]\n",
    "\n",
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return meanv([w.vector for w in sent])\n",
    "\n",
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sentvec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine(np.mean([w.vector for w in x], axis=0), input_vec),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec('dog'), vec('puppy')) > cosine(vec('trousers'), vec('pants'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.85852146, 0.8541468)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec('dog'), vec('puppy')), cosine(vec('trousers'), vec('pants'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pudding',\n",
       " 'bread',\n",
       " 'chocolate',\n",
       " 'butter',\n",
       " 'buttered',\n",
       " 'sauce',\n",
       " 'cream',\n",
       " 'shortbread',\n",
       " 'tasty',\n",
       " 'hearty']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parrish\n",
    "spacy_closest(tokens1, vec(\"pie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bread',\n",
       " 'chocolate',\n",
       " 'bake',\n",
       " 'butter',\n",
       " 'hearty',\n",
       " 'gravy',\n",
       " 'apples',\n",
       " 'greased',\n",
       " 'filling',\n",
       " 'saucer']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens2, vec(\"pie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cake',\n",
       " 'recipe',\n",
       " 'bread',\n",
       " 'chocolate',\n",
       " 'crusts',\n",
       " 'cakes',\n",
       " 'pancakes',\n",
       " 'biscuit',\n",
       " 'bacon',\n",
       " 'risotto']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens5, vec(\"pie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_list = [w.text for w in ch1 if w.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17393, 2486)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch1_list), len(set(ch1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the            1155\n",
       "and             774\n",
       "of              430\n",
       "to              391\n",
       "was             383\n",
       "a               337\n",
       "he              326\n",
       "his             323\n",
       "in              300\n",
       "had             222\n",
       "said            205\n",
       "that            196\n",
       "He              172\n",
       "it              167\n",
       "him             142\n",
       "on              136\n",
       "were            133\n",
       "not             119\n",
       "with            108\n",
       "you             107\n",
       "for             104\n",
       "they             98\n",
       "The              98\n",
       "at               95\n",
       "be               91\n",
       "Mr               84\n",
       "out              84\n",
       "would            83\n",
       "up               80\n",
       "It               76\n",
       "               ... \n",
       "generals          1\n",
       "soon              1\n",
       "flowerbeds        1\n",
       "twined            1\n",
       "reply             1\n",
       "inclined          1\n",
       "Xavier            1\n",
       "Lavender          1\n",
       "guards            1\n",
       "arguing           1\n",
       "reddened          1\n",
       "finger            1\n",
       "dropped           1\n",
       "backhand          1\n",
       "pinned            1\n",
       "chest             1\n",
       "crashing          1\n",
       "poor              1\n",
       "hushed            1\n",
       "aspirations       1\n",
       "hurt              1\n",
       "nobody            1\n",
       "Napoleon          1\n",
       "wothe             1\n",
       "underneath        1\n",
       "rods              1\n",
       "rolling           1\n",
       "enemy             1\n",
       "tig               1\n",
       "eldest            1\n",
       "Length: 2486, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need to remove stop words\n",
    "pd.Series(ch1_list).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grass',\n",
       " 'green',\n",
       " 'Green',\n",
       " 'yellow',\n",
       " 'Yellow',\n",
       " 'red',\n",
       " 'lawn',\n",
       " 'blue',\n",
       " 'leaf',\n",
       " 'brown']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blue_to_sky = subtractv(vec(\"blue\"), vec(\"sky\"))\n",
    "spacy_closest(tokens5, addv(blue_to_sky, vec(\"grass\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’d go\n",
      "straight up to the rector and tell him about it after dinner.\n",
      "\n",
      "\n",
      "---\n",
      "—Father Dolan came in today and pandied me because I was not writing my\n",
      "theme.\n",
      "\n",
      "\n",
      "---\n",
      "Dante turned on her and said:\n",
      "\n",
      "—And am I to sit here and listen to the pastors of my church being\n",
      "flouted?\n",
      "\n",
      "\n",
      "---\n",
      "I’d go up and\n",
      "tell the rector on him.\n",
      "\n",
      "\n",
      "---\n",
      "But he said:\n",
      "\n",
      "—Gentlemen, the happiest day of my life was the day on which I made my\n",
      "first holy communion.\n",
      "\n",
      "\n",
      "---\n",
      "There was a cold night smell in the chapel.\n",
      "---\n",
      "It was lovely to be tired.\n",
      "---\n",
      "Brother Michael was very decent and always told him the news out of the\n",
      "paper they got every day up in the castle.\n",
      "---\n",
      "That never went to mass.\n",
      "\n",
      "\n",
      "---\n",
      "And\n",
      "that night Mr Casey had not gone to Dublin by train but a car had come\n",
      "to the door\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sents1 = list(ch1.sents)\n",
    "\n",
    "for sent in spacy_closest_sent(sents1, \"I went to church this morning.\"):\n",
    "    print(sent.text)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight workflow\n",
    "### Data cleaning:\n",
    "- tokenize\n",
    "- remove non-alphanumeric\n",
    "- to lower case\n",
    "- combine words with similar spellings\n",
    "- lemmatize?\n",
    "\n",
    "### Words into numbers\n",
    "- bag words [0 0 0 0 0 1 0 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
