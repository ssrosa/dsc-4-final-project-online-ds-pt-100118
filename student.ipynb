{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Steven Rosa\n",
    "* Student pace: part time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Jeff Herman\n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work flows\n",
    "#### Parrish\n",
    "https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469\n",
    "\n",
    "#### Graph convolutional network (\"semi-supervised\")\n",
    "https://towardsdatascience.com/text-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gutenberg.org/ebooks/4217'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "#Learn workflow\n",
    "from nltk.corpus import stopwords\n",
    "import string #for removing punctuation from text\n",
    "from nltk import word_tokenize #Another way of tokenizing\n",
    "\n",
    "\n",
    "#Parrish workflow\n",
    "from numpy import dot #parrish\n",
    "from numpy.linalg import norm #parrish\n",
    "import en_core_web_md #parrish\n",
    "import spacy #parrish \n",
    "from __future__ import unicode_literals #parrish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Stee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Needed for first time use on each machine:\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords: \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "import string\n",
    "\n",
    "# Now we have a list that includes all english stopwords, as well as all punctuation\n",
    "stopwords_list += list(string.punctuation)\n",
    "\n",
    "#https://learn.co/tracks/data-science-career-v1-1/module-4-advanced-machine-learning-deep-learning/section-37-foundations-of-natural-language-processing-nlp/feature-engineering-for-text-data#\n",
    "\n",
    "#&c &c &c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files (without nlp module)\n",
    "with open('chapter1.txt', 'r', encoding = 'utf-8') as f1, \\\n",
    "    open('chapter2.txt', 'r', encoding = 'utf-8') as f2, \\\n",
    "    open('chapter3.txt', 'r', encoding = 'utf-8') as f3, \\\n",
    "    open('chapter4.txt', 'r', encoding = 'utf-8') as f4, \\\n",
    "    open('chapter5.txt', 'r', encoding = 'utf-8') as f5:\n",
    "    ch1 = f1.read()\n",
    "    ch2 = f2.read()\n",
    "    ch3 = f3.read()\n",
    "    ch4 = f4.read()\n",
    "    ch5 = f5.read()\n",
    "\n",
    "#tokenize all\n",
    "tokens1 = word_tokenize(ch1)\n",
    "tokens2 = word_tokenize(ch2)\n",
    "tokens3 = word_tokenize(ch3)\n",
    "tokens4 = word_tokenize(ch4)\n",
    "tokens5 = word_tokenize(ch5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why does this not work\n",
    "tokens1 = word_tokenize(ch1)\n",
    "stopped_tokens1 = [w.lower() for w in tokens1 if w not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "said             205\n",
       "’                179\n",
       "he               172\n",
       "the               98\n",
       "mr                84\n",
       "would             80\n",
       "father            80\n",
       "it                75\n",
       "stephen           72\n",
       "i                 70\n",
       "and               63\n",
       "like              61\n",
       "but               60\n",
       "could             58\n",
       "dedalus           58\n",
       "fellows           52\n",
       "little            49\n",
       "dante             49\n",
       "prefect           46\n",
       "one               45\n",
       "they              44\n",
       "fellow            43\n",
       "face              40\n",
       "made              39\n",
       "day               39\n",
       "eyes              39\n",
       "rector            38\n",
       "casey             37\n",
       "god               35\n",
       "cold              35\n",
       "                ... \n",
       "unlocking          1\n",
       "daly               1\n",
       "crossed            1\n",
       "brass              1\n",
       "toasted            1\n",
       "hurroo             1\n",
       "crumple            1\n",
       "flock              1\n",
       "_—i                1\n",
       "plop               1\n",
       "hogwash            1\n",
       "dishonour          1\n",
       "_for               1\n",
       "drop               1\n",
       "bump               1\n",
       "—oh                1\n",
       "—                  1\n",
       "stale              1\n",
       "_priesthunter      1\n",
       "twinkled           1\n",
       "lawn               1\n",
       "alleghanies        1\n",
       "tolling            1\n",
       "generals           1\n",
       "ghosts             1\n",
       "asleep             1\n",
       "seven              1\n",
       "bearded            1\n",
       "ferulæ             1\n",
       "dancing            1\n",
       "Length: 2421, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(stopped_tokens1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parrish workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chapter1.txt', 'r', encoding = 'utf-8') as f1, \\\n",
    "    open('chapter2.txt', 'r', encoding = 'utf-8') as f2, \\\n",
    "    open('chapter3.txt', 'r', encoding = 'utf-8') as f3, \\\n",
    "    open('chapter4.txt', 'r', encoding = 'utf-8') as f4, \\\n",
    "    open('chapter5.txt', 'r', encoding = 'utf-8') as f5:\n",
    "    ch1 = nlp(f1.read())\n",
    "    ch2 = nlp(f2.read())\n",
    "    ch3 = nlp(f3.read())\n",
    "    ch4 = nlp(f4.read())\n",
    "    ch5 = nlp(f5.read())\n",
    "\n",
    "tokens1 = list(set([w.text for w in ch1 if w.is_alpha]))\n",
    "tokens2 = list(set([w.text for w in ch2 if w.is_alpha]))\n",
    "tokens3 = list(set([w.text for w in ch3 if w.is_alpha]))\n",
    "tokens4 = list(set([w.text for w in ch4 if w.is_alpha]))\n",
    "tokens5 = list(set([w.text for w in ch5 if w.is_alpha]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Parrish\n",
    "nlp.vocab['cheese'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions from Parrish\n",
    "\n",
    "\n",
    "def subtractv(coord1, coord2):\n",
    "    return [c1 - c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "def addv(coord1, coord2):\n",
    "    return [c1 + c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "def meanv(coords):\n",
    "    # assumes every item in coords has same length as item 0\n",
    "    sumv = [0] * len(coords[0])\n",
    "    for item in coords:\n",
    "        for i in range(len(item)):\n",
    "            sumv[i] += item[i]\n",
    "    mean = [0] * len(sumv)\n",
    "    for i in range(len(sumv)):\n",
    "        mean[i] = float(sumv[i]) / len(coords)\n",
    "    return mean\n",
    "\n",
    "\n",
    "#Convenience function\n",
    "def vec(s):\n",
    "    return nlp.vocab[s].vector\n",
    "\n",
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def spacy_closest(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine(vec_to_check, vec(x)),\n",
    "                  reverse=True)[:n]\n",
    "\n",
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return meanv([w.vector for w in sent])\n",
    "\n",
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sentvec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine(np.mean([w.vector for w in x], axis=0), input_vec),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec('dog'), vec('puppy')) > cosine(vec('trousers'), vec('pants'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec('dog'), vec('puppy')), cosine(vec('trousers'), vec('pants'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parrish\n",
    "spacy_closest(tokens1, vec(\"pie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens2, vec(\"pie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens5, vec(\"pie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_list = [w.text for w in ch1 if w.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ch1_list), len(set(ch1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to remove stop words\n",
    "pd.Series(ch1_list).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_to_sky = subtractv(vec(\"blue\"), vec(\"sky\"))\n",
    "spacy_closest(tokens5, addv(blue_to_sky, vec(\"grass\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sents1 = list(ch1.sents)\n",
    "\n",
    "for sent in spacy_closest_sent(sents1, \"I went to church this morning.\"):\n",
    "    print(sent.text)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight workflow\n",
    "### Data cleaning:\n",
    "- tokenize\n",
    "- remove non-alphanumeric\n",
    "- to lower case\n",
    "- combine words with similar spellings\n",
    "- lemmatize?\n",
    "\n",
    "### Words into numbers\n",
    "- bag words [0 0 0 0 0 1 0 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
