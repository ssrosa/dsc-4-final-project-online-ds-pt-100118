{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Steven Rosa\n",
    "* Student pace: part time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Jeff Herman\n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work flows\n",
    "#### Parrish\n",
    "https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469\n",
    "\n",
    "#### Graph convolutional network (\"semi-supervised\")\n",
    "https://towardsdatascience.com/text-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gutenberg.org/ebooks/4217'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "#Learn workflow\n",
    "from nltk.corpus import stopwords\n",
    "import string #for removing punctuation from text\n",
    "from nltk import word_tokenize #Another way of tokenizing\n",
    "from nltk import FreqDist\n",
    "\n",
    "#Parrish workflow\n",
    "from numpy import dot #parrish\n",
    "from numpy.linalg import norm #parrish\n",
    "import en_core_web_md #parrish\n",
    "import spacy #parrish \n",
    "from __future__ import unicode_literals #parrish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Stee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Needed for first time use on each machine:\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "\n",
    "# Now we have a list that includes all english stopwords, as well as all punctuation\n",
    "stopwords_list += list(string.punctuation)\n",
    "\n",
    "#https://learn.co/tracks/data-science-career-v1-1/module-4-advanced-machine-learning-deep-learning/section-37-foundations-of-natural-language-processing-nlp/feature-engineering-for-text-data#\n",
    "\n",
    "#&c &c &c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files (without nlp module)\n",
    "with open('chapter1.txt', 'r', encoding = 'utf-8') as f1, \\\n",
    "    open('chapter2.txt', 'r', encoding = 'utf-8') as f2, \\\n",
    "    open('chapter3.txt', 'r', encoding = 'utf-8') as f3, \\\n",
    "    open('chapter4.txt', 'r', encoding = 'utf-8') as f4, \\\n",
    "    open('chapter5.txt', 'r', encoding = 'utf-8') as f5:\n",
    "    ch1 = f1.read()\n",
    "    ch2 = f2.read()\n",
    "    ch3 = f3.read()\n",
    "    ch4 = f4.read()\n",
    "    ch5 = f5.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize all\n",
    "tokens1 = word_tokenize(ch1)\n",
    "tokens2 = word_tokenize(ch2)\n",
    "tokens3 = word_tokenize(ch3)\n",
    "tokens4 = word_tokenize(ch4)\n",
    "tokens5 = word_tokenize(ch5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why does this not work\n",
    "stopped_tokens1 = [w.lower() for w in tokens1 if w.lower() not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'he' in stopped_tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist1 = FreqDist(stopped_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 205),\n",
       " ('’', 179),\n",
       " ('mr', 84),\n",
       " ('father', 80),\n",
       " ('would', 80),\n",
       " ('stephen', 72),\n",
       " ('like', 61),\n",
       " ('dedalus', 58),\n",
       " ('could', 58),\n",
       " ('fellows', 52),\n",
       " ('little', 49),\n",
       " ('dante', 49),\n",
       " ('prefect', 46),\n",
       " ('one', 45),\n",
       " ('fellow', 43),\n",
       " ('face', 40),\n",
       " ('eyes', 39),\n",
       " ('day', 39),\n",
       " ('made', 39),\n",
       " ('rector', 38),\n",
       " ('casey', 37),\n",
       " ('cold', 35),\n",
       " ('god', 35),\n",
       " ('dark', 33),\n",
       " ('go', 32),\n",
       " ('away', 32),\n",
       " ('mother', 31),\n",
       " ('felt', 31),\n",
       " ('cried', 31),\n",
       " ('fleming', 31),\n",
       " ('looked', 30),\n",
       " ('hand', 30),\n",
       " ('went', 30),\n",
       " ('name', 29),\n",
       " ('right', 29),\n",
       " ('back', 28),\n",
       " ('door', 28),\n",
       " ('told', 26),\n",
       " ('hands', 26),\n",
       " ('asked', 26),\n",
       " ('think', 26),\n",
       " ('wells', 26),\n",
       " ('studies', 26),\n",
       " ('first', 25),\n",
       " ('smell', 25),\n",
       " ('come', 25),\n",
       " ('air', 25),\n",
       " ('arnall', 25),\n",
       " ('voice', 25),\n",
       " ('know', 25),\n",
       " ('head', 24),\n",
       " ('us', 24),\n",
       " ('say', 24),\n",
       " ('old', 24),\n",
       " ('tell', 24),\n",
       " ('came', 23),\n",
       " ('—i', 23),\n",
       " ('night', 22),\n",
       " ('two', 21),\n",
       " ('see', 21),\n",
       " ('white', 21),\n",
       " ('sir', 21),\n",
       " ('heard', 20),\n",
       " ('people', 20),\n",
       " ('saw', 20),\n",
       " ('long', 20),\n",
       " ('brother', 20),\n",
       " ('time', 19),\n",
       " ('bed', 19),\n",
       " ('uncle', 19),\n",
       " ('charles', 19),\n",
       " ('round', 19),\n",
       " ('green', 18),\n",
       " ('every', 18),\n",
       " ('big', 18),\n",
       " ('thought', 18),\n",
       " ('along', 17),\n",
       " ('michael', 17),\n",
       " ('table', 17),\n",
       " ('way', 17),\n",
       " ('athy', 17),\n",
       " ('put', 16),\n",
       " ('red', 16),\n",
       " ('home', 16),\n",
       " ('turned', 16),\n",
       " ('always', 16),\n",
       " ('got', 16),\n",
       " ('mrs', 16),\n",
       " ('place', 15),\n",
       " ('queer', 15),\n",
       " ('different', 15),\n",
       " ('going', 15),\n",
       " ('line', 15),\n",
       " ('called', 15),\n",
       " ('side', 15),\n",
       " ('nice', 15),\n",
       " ('hot', 15),\n",
       " ('noise', 15),\n",
       " ('let', 15),\n",
       " ('—yes', 15),\n",
       " ('perhaps', 14),\n",
       " ('simon', 14),\n",
       " ('hear', 14),\n",
       " ('began', 14),\n",
       " ('must', 14),\n",
       " ('pain', 14),\n",
       " ('still', 14),\n",
       " ('soft', 14),\n",
       " ('make', 14),\n",
       " ('john', 14),\n",
       " ('dolan', 14),\n",
       " ('boy', 13),\n",
       " ('...', 13),\n",
       " ('rose', 13),\n",
       " ('boys', 13),\n",
       " ('fire', 13),\n",
       " ('water', 13),\n",
       " ('knew', 13),\n",
       " ('feel', 13),\n",
       " ('fingers', 13),\n",
       " ('get', 13),\n",
       " ('strange', 13),\n",
       " ('well', 13),\n",
       " ('chapel', 13),\n",
       " ('—you', 13),\n",
       " ('good', 12),\n",
       " ('—o', 12),\n",
       " ('refectory', 12),\n",
       " ('answer', 12),\n",
       " ('castle', 12),\n",
       " ('look', 12),\n",
       " ('far', 12),\n",
       " ('held', 12),\n",
       " ('opened', 12),\n",
       " ('saying', 12),\n",
       " ('quickly', 12),\n",
       " ('end', 12),\n",
       " ('stood', 12),\n",
       " ('glasses', 12),\n",
       " ('broke', 12),\n",
       " ('—and', 12),\n",
       " ('warm', 11),\n",
       " ('grey', 11),\n",
       " ('—what', 11),\n",
       " ('speak', 11),\n",
       " ('hall', 11),\n",
       " ('study', 11),\n",
       " ('mean', 11),\n",
       " ('looking', 11),\n",
       " ('tried', 11),\n",
       " ('yes', 11),\n",
       " ('middle', 11),\n",
       " ('class', 11),\n",
       " ('bad', 11),\n",
       " ('man', 11),\n",
       " ('holy', 11),\n",
       " ('great', 11),\n",
       " ('pandybat', 11),\n",
       " ('cruel', 11),\n",
       " ('evening', 10),\n",
       " ('light', 10),\n",
       " ('weak', 10),\n",
       " ('third', 10),\n",
       " ('nasty', 10),\n",
       " ('roche', 10),\n",
       " ('thunder', 10),\n",
       " ('wanted', 10),\n",
       " ('wondered', 10),\n",
       " ('mouth', 10),\n",
       " ('closed', 10),\n",
       " ('moonan', 10),\n",
       " ('sound', 10),\n",
       " ('slowly', 10),\n",
       " ('corridor', 10),\n",
       " ('sat', 10),\n",
       " ('another', 10),\n",
       " ('heart', 10),\n",
       " ('train', 10),\n",
       " ('walking', 10),\n",
       " ('moment', 10),\n",
       " ('grammar', 10),\n",
       " ('staircase', 10),\n",
       " ('priests', 10),\n",
       " ('—well', 10),\n",
       " ('silence', 10),\n",
       " ('across', 10),\n",
       " ('parnell', 9),\n",
       " ('answered', 9),\n",
       " ('give', 9),\n",
       " ('cecil', 9),\n",
       " ('never', 9),\n",
       " ('bent', 9),\n",
       " ('something', 9),\n",
       " ('book', 9),\n",
       " ('lovely', 9),\n",
       " ('last', 9),\n",
       " ('thing', 9),\n",
       " ('passed', 9),\n",
       " ('beautiful', 9),\n",
       " ('towards', 9)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqdist1.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parrish workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chapter1.txt', 'r', encoding = 'utf-8') as f1, \\\n",
    "    open('chapter2.txt', 'r', encoding = 'utf-8') as f2, \\\n",
    "    open('chapter3.txt', 'r', encoding = 'utf-8') as f3, \\\n",
    "    open('chapter4.txt', 'r', encoding = 'utf-8') as f4, \\\n",
    "    open('chapter5.txt', 'r', encoding = 'utf-8') as f5:\n",
    "    ch1 = nlp(f1.read())\n",
    "    ch2 = nlp(f2.read())\n",
    "    ch3 = nlp(f3.read())\n",
    "    ch4 = nlp(f4.read())\n",
    "    ch5 = nlp(f5.read())\n",
    "\n",
    "tokens1 = list(set([w.text for w in ch1 if w.is_alpha]))\n",
    "tokens2 = list(set([w.text for w in ch2 if w.is_alpha]))\n",
    "tokens3 = list(set([w.text for w in ch3 if w.is_alpha]))\n",
    "tokens4 = list(set([w.text for w in ch4 if w.is_alpha]))\n",
    "tokens5 = list(set([w.text for w in ch5 if w.is_alpha]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Parrish\n",
    "nlp.vocab['cheese'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions from Parrish\n",
    "\n",
    "\n",
    "def subtractv(coord1, coord2):\n",
    "    return [c1 - c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "def addv(coord1, coord2):\n",
    "    return [c1 + c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "def meanv(coords):\n",
    "    # assumes every item in coords has same length as item 0\n",
    "    sumv = [0] * len(coords[0])\n",
    "    for item in coords:\n",
    "        for i in range(len(item)):\n",
    "            sumv[i] += item[i]\n",
    "    mean = [0] * len(sumv)\n",
    "    for i in range(len(sumv)):\n",
    "        mean[i] = float(sumv[i]) / len(coords)\n",
    "    return mean\n",
    "\n",
    "\n",
    "#Convenience function\n",
    "def vec(s):\n",
    "    return nlp.vocab[s].vector\n",
    "\n",
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def spacy_closest(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine(vec_to_check, vec(x)),\n",
    "                  reverse=True)[:n]\n",
    "\n",
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return meanv([w.vector for w in sent])\n",
    "\n",
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sentvec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine(np.mean([w.vector for w in x], axis=0), input_vec),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec('dog'), vec('puppy')) > cosine(vec('trousers'), vec('pants'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec('dog'), vec('puppy')), cosine(vec('trousers'), vec('pants'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parrish\n",
    "spacy_closest(tokens1, vec(\"pie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens2, vec(\"pie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tokens5, vec(\"pie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_list = [w.text for w in ch1 if w.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ch1_list), len(set(ch1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to remove stop words\n",
    "pd.Series(ch1_list).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_to_sky = subtractv(vec(\"blue\"), vec(\"sky\"))\n",
    "spacy_closest(tokens5, addv(blue_to_sky, vec(\"grass\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sents1 = list(ch1.sents)\n",
    "\n",
    "for sent in spacy_closest_sent(sents1, \"I went to church this morning.\"):\n",
    "    print(sent.text)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight workflow\n",
    "### Data cleaning:\n",
    "- tokenize\n",
    "- remove non-alphanumeric\n",
    "- to lower case\n",
    "- combine words with similar spellings\n",
    "- lemmatize?\n",
    "\n",
    "### Words into numbers\n",
    "- bag words [0 0 0 0 0 1 0 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
