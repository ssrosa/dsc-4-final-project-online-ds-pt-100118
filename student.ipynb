{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Steven Rosa\n",
    "* Student pace: part time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: Jeff Herman\n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work flows\n",
    "#### Parrish\n",
    "https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469\n",
    "\n",
    "#### Graph convolutional network (\"semi-supervised\")\n",
    "https://towardsdatascience.com/text-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gutenberg.org/ebooks/4217'\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D #For viewing 3d pca feature plots\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA #For squashing word vector means\n",
    "from sklearn.preprocessing import StandardScaler #For normalizing data\n",
    "#To divide data when testing a trained model\n",
    "from sklearn.model_selection import train_test_split \n",
    "#To build regular logistic regression models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "##################\n",
    "# ASSESSING MODELS:\n",
    "#To assess accuracy of logistic regression or decision trees\n",
    "from sklearn.metrics import confusion_matrix \n",
    "#To iteratively append labels to cells in a confusion matrix\n",
    "import itertools \n",
    "#To get accuracy, precision, recall, and F1 score (weighted accuracy) of a given confusion matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#To view the accuracy metrics for a given confusion matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Learn workflow\n",
    "from nltk.corpus import stopwords\n",
    "import string #for removing punctuation from text\n",
    "from nltk import word_tokenize #Another way of tokenizing\n",
    "from nltk import FreqDist\n",
    "\n",
    "from pandasql import sqldf #for manipulating DataFrames with SQL style queries\n",
    "\n",
    "#Parrish workflow\n",
    "from numpy import dot #parrish\n",
    "from numpy.linalg import norm #parrish\n",
    "import en_core_web_md #parrish\n",
    "import spacy #parrish \n",
    "from __future__ import unicode_literals #parrish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One off library imports here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn.co workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed for first time use on each machine:\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files (without nlp module)\n",
    "with open('chapter1.txt', 'r', encoding = 'utf-8') as f1, \\\n",
    "    open('chapter2.txt', 'r', encoding = 'utf-8') as f2, \\\n",
    "    open('chapter3.txt', 'r', encoding = 'utf-8') as f3, \\\n",
    "    open('chapter4.txt', 'r', encoding = 'utf-8') as f4, \\\n",
    "    open('chapter5.txt', 'r', encoding = 'utf-8') as f5:\n",
    "    ch1 = f1.read()\n",
    "    ch2 = f2.read()\n",
    "    ch3 = f3.read()\n",
    "    ch4 = f4.read()\n",
    "    ch5 = f5.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize all\n",
    "tokens1 = word_tokenize(ch1)\n",
    "tokens2 = word_tokenize(ch2)\n",
    "tokens3 = word_tokenize(ch3)\n",
    "tokens4 = word_tokenize(ch4)\n",
    "tokens5 = word_tokenize(ch5)\n",
    "all_tokens = [tokens1, tokens2, tokens3, tokens4, tokens5]\n",
    "full_text = []\n",
    "full_text = [full_text.extend(tokens) for tokens in all_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_ch1 = stop(tokens1)\n",
    "stopped_ch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_ch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopped = {'Chapter_' + str(i + 1): stop(tokens) \\\n",
    "                for i, tokens in enumerate(all_tokens)}\n",
    "#Get list of all stopped words in corpus\n",
    "all_stopped = []\n",
    "for key, val in stopped.items():\n",
    "    all_stopped.extend(val)\n",
    "    \n",
    "fds = {key: FreqDist(stopped[key]) for key, val in stopped.items()}\n",
    "all_fd = FreqDist(all_stopped)\n",
    "\n",
    "#Use frequency distributions from each column to construct \n",
    "#a DataFrame listing frequency of every word across every chapter\n",
    "word_counts = pd.DataFrame(list(all_fd.items()), columns = ['Word','Total'])\n",
    "\n",
    "for key, val in fds.items():\n",
    "    word_counts[key] = [val[word] if val[word] else 0 \\\n",
    "                  for word in word_counts['Word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get counts for every token in each chapter\n",
    "def word_counts_table(df, column):\n",
    "    '''\n",
    "    '''\n",
    "    q = ''' SELECT *\n",
    "            FROM {} df\n",
    "            ORDER BY df.{} DESC\n",
    "    ;'''.format(df, column)\n",
    "    results = pysqldf(q)\n",
    "    print(q)\n",
    "    return results\n",
    "\n",
    "def word_percents_table(df, chapters, order_by):\n",
    "    '''\n",
    "    Parameters:\n",
    "    chapters (list) list(fds.keys())\n",
    "    '''\n",
    "    #Solve fence post comma problem by building different string if key is last in list\n",
    "    select = ''.join(['''(df.''' + key + ' * 1000 / df.Total) * 0.1 ' + key + ''', \n",
    "        ''' if key != chapters[-1] else \\\n",
    "        '(df.' + key + ' * 1000 / df.Total) * 0.1 ' + key + ''' \n",
    "        ''' for key in chapters])\n",
    "    \n",
    "    q = '''SELECT df.Word, df.Total, \n",
    "        '''\n",
    "    q += select\n",
    "    q += '''\n",
    "        FROM {} df\n",
    "        ;'''.format(df, order_by)\n",
    "    results = pysqldf(q)\n",
    "    print(q)\n",
    "    return results\n",
    "\n",
    "def select_percent_lim(df, percent, lim, chapters):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    select = ''.join([\n",
    "        'SUM(CASE WHEN df.' + key + ' > ' + str(percent) + \\\n",
    "        ' AND df.Total > ' + str(lim) + \\\n",
    "        ' THEN 1 ELSE 0 END) ' + key + ''', \n",
    "        ''' if key != chapters[-1] else \\\n",
    "        'SUM(CASE WHEN df.' + key + ' > ' + str(percent) + \\\n",
    "        ' AND df.Total > ' + str(lim) + \\\n",
    "        ' THEN 1 ELSE 0 END) '  + key + ''' \n",
    "        ''' for key in chapters])\n",
    "    q = 'SELECT '\n",
    "    q += select\n",
    "    q += '''FROM {} df\n",
    "    ;'''.format(df)\n",
    "    results = pysqldf(q)\n",
    "    print(q)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A lambda function with the \"globals\" so I don't have to pass them in every time\n",
    "pysqldf = lambda q: sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_ordered = word_counts_table('word_counts', 'Chapter_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_ordered.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percents = word_percents_table('word_counts', list(fds.keys()), 'Chapter_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percents.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of words in each chapter\n",
    "len(stopped['Chapter_1']), len(stopped['Chapter_2']), len(stopped['Chapter_3']), len(stopped['Chapter_4']), len(stopped['Chapter_5'])\n",
    "sum([len(stopped['Chapter_1']), len(stopped['Chapter_2']), len(stopped['Chapter_3']), len(stopped['Chapter_4']), len(stopped['Chapter_5'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If plain vanilla model can't get higher than 30% accuracy then bigger classes are having outsized effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of words making 99% of their appearances in a given chapter\n",
    "words90 = pd.DataFrame(columns = list(fds.keys()))\n",
    "for i in range(40):\n",
    "    words90 = words99.append(select_percent_lim('percents', 90,\n",
    "                                                i, \n",
    "                                                list(fds.keys())),\n",
    "                                                ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use to check accuracy of SQL queries\n",
    "#See all the words that appear in a given chapter 99% out of all occurences\n",
    "percents[percents['Chapter_4'] > 99][['Word', 'Total']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parrish workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "with open('chapter1.txt', 'r', encoding = 'utf-8') as f1, \\\n",
    "    open('chapter2.txt', 'r', encoding = 'utf-8') as f2, \\\n",
    "    open('chapter3.txt', 'r', encoding = 'utf-8') as f3, \\\n",
    "    open('chapter4.txt', 'r', encoding = 'utf-8') as f4, \\\n",
    "    open('chapter5.txt', 'r', encoding = 'utf-8') as f5:\n",
    "    ch1 = nlp(f1.read())\n",
    "    ch2 = nlp(f2.read())\n",
    "    ch3 = nlp(f3.read())\n",
    "    ch4 = nlp(f4.read())\n",
    "    ch5 = nlp(f5.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example from chapter 3 for possible visualization\n",
    "'I went to church this morning\n",
    "'He could still leave the chapel.''\n",
    "I/he went/leave to/the church/chapel still/morning\n",
    "I he you they she we\n",
    "went go come leave \n",
    "to toward at the whatever\n",
    "church chapel temple basilica\n",
    "still morning evening yet now later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "sv_df1 = sent_vec_df(ch1, 1)\n",
    "sv_df2 = sent_vec_df(ch2, 2)\n",
    "sv_df3 = sent_vec_df(ch3, 3)\n",
    "sv_df4 = sent_vec_df(ch4, 4)\n",
    "sv_df5 = sent_vec_df(ch5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After dropping nulls\n",
    "sv_df1.shape[0], sv_df2.shape[0], sv_df3.shape[0], sv_df4.shape[0], sv_df5.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box and whisker plots for features: char_count, mean token len, token count\n",
    "## change df to sv_df later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box and whisker plots\n",
    "#Widths of boxes are lengths of chapters in proportion to Chapter 5.\n",
    "box_widths = [df1['char_count'].shape[0] / df5['char_count'].shape[0],\n",
    " df2['char_count'].shape[0] / df5['char_count'].shape[0],\n",
    " df3['char_count'].shape[0] / df5['char_count'].shape[0],\n",
    " df4['char_count'].shape[0] / df5['char_count'].shape[0],\n",
    " df5['char_count'].shape[0] / df5['char_count'].shape[0]\n",
    "]\n",
    "#Labels for box and whisker plots\n",
    "box_labels = ['Chapter 1', 'Chapter 2', 'Chapter 3', 'Chapter 4', 'Chapter 5']\n",
    "\n",
    "#First box and whisker plot\n",
    "#Chapter 4 has the longest sentences overall.\n",
    "#Widths of boxes are lengths of chapters in proportion to Chapter 5.\n",
    "\n",
    "fig = plt.figure(figsize = (12, 8))\n",
    "plt.boxplot([df1['char_count'],\n",
    "             df2['char_count'],\n",
    "             df3['char_count'],\n",
    "             df4['char_count'],\n",
    "             df5['char_count']\n",
    "            ], \n",
    "            labels = box_labels,\n",
    "            widths = box_widths\n",
    "           )\n",
    "plt.legend()\n",
    "plt.title('Character counts of each sentence, by chapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 8))\n",
    "plt.boxplot([df1['mean_token_len'],\n",
    "             df2['mean_token_len'],\n",
    "             df3['mean_token_len'],\n",
    "             df4['mean_token_len'],\n",
    "             df5['mean_token_len']\n",
    "            ], \n",
    "            labels = box_labels,\n",
    "            widths = box_widths)\n",
    "plt.legend()\n",
    "plt.title('Mean token length for each sentence, by chapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 8))\n",
    "plt.boxplot([df1['token_count'],\n",
    "             df2['token_count'],\n",
    "             df3['token_count'],\n",
    "             df4['token_count'],\n",
    "             df5['token_count']\n",
    "            ], \n",
    "            labels = box_labels,\n",
    "            widths = box_widths)\n",
    "plt.title('Token count of each sentence, by chapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observations from box plots: Chapter 4 has longer character counts of its sentences\n",
    "#and longer token counts (lengths of its words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis: 300 vectors down to 3\n",
    "## Sentence 'mean vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape[0], df2.shape[0], df3.shape[0], df4.shape[0], df5.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First look at sentence vectors, not normalized\n",
    "sv_df = sv_df1[['tokens', 'mean_vector', 'label']].append([\n",
    "                                                    sv_df1[['tokens', 'mean_vector', 'label']], \n",
    "                                                    sv_df1[['tokens', 'mean_vector', 'label']],\n",
    "                                                    sv_df1[['tokens', 'mean_vector', 'label']],\n",
    "                                                    sv_df1[['tokens', 'mean_vector', 'label']]\n",
    "                                                    ], ignore_index = True)\n",
    "#Rename columns to work with pca function\n",
    "sv_df1.rename({'mean_vector': 'vector', 'tokens': 'token'}, axis = 1, inplace = True)\n",
    "#Get PCA for this df\n",
    "df_pca = do_pca(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = [-3, 4, -2.5, 2, -3, 3]\n",
    "title = 'Sentence vectors by chapter (not normalized)'\n",
    "plot_list = [{1: 'red', 2: 'orange', 3: 'yellow', 4: 'green', 5: 'purple'}]\n",
    "\n",
    "#Plot chapters together to see distribution of PCA in 3d\n",
    "plot_pca(df_pca, plot_list, xyz = xyz, figsize = (14,10), \n",
    "          legend = True, title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at individual plots after spotting a green cluster at azim 60 elev -90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_list = [{1: 'red'}, {2: 'orange'}, {3: 'yellow'}, {4: 'green'}, {5: 'purple'}]\n",
    "xyz = [-2, 5, -3, 3, -2, 4]\n",
    "\n",
    "#Draw separate plots for sentence vectors in each chapter\n",
    "plot_pca(df_pca, plot_list, xyz = xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with PCA = 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual  word tokens as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_df1 = word_vec_df(ch1, 1)\n",
    "wv_df2 = word_vec_df(ch2, 2)\n",
    "wv_df3 = word_vec_df(ch3, 3)\n",
    "wv_df4 = word_vec_df(ch4, 4)\n",
    "wv_df5 = word_vec_df(ch5, 5)\n",
    "wv_df = wv_df1.append([wv_df2, wv_df3, wv_df4, wv_df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRevious ones with old code\n",
    "wv_df1.shape[0], wv_df1.shape[0], wv_df1.shape[0], wv_df1.shape[0], wv_df1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_df1.shape[0], wv_df2.shape[0], wv_df3.shape[0], wv_df4.shape[0], wv_df5.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "byword_pca = do_pca(wv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot PCA of word vectors together\n",
    "plot_list = [{1: 'red', 2: 'orange', 3: 'yellow', 4: 'green', 5: 'purple'}]\n",
    "\n",
    "plot_pca(byword_pca, plot_list, figsize = (12, 8), title = 'PCA of word vectors, not normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show each chapter as its own plot\n",
    "plot_list = [{1: 'red'}, {2: 'orange'}, {3: 'yellow'}, {4: 'green'}, {5: 'purple'}]\n",
    "\n",
    "plot_pca(byword_pca, plot_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting all word vectors yields basically a big blob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about just the empty vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empties = wv_df[[True if np.all(row == zeros) else False for row in wv_df['vector']]]\n",
    "\n",
    "empty_pca = do_pca(empties)\n",
    "empty_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all get coordinates of 0 0 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will unique words look like anything when plotted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros(300)\n",
    "\n",
    "unique1 = appears_in(wv_df, wv_df1, 1)\n",
    "unique2 = appears_in(wv_df, wv_df2, 2)\n",
    "unique3 = appears_in(wv_df, wv_df3, 3)\n",
    "unique4 = appears_in(wv_df, wv_df4, 4)\n",
    "unique5 = appears_in(wv_df, wv_df5, 5)\n",
    "uniq1_notna = unique1[[False if np.all(row == zeros) else True for row in unique1['vector']]]\n",
    "uniq2_notna = unique2[[False if np.all(row == zeros) else True for row in unique2['vector']]]\n",
    "uniq3_notna = unique3[[False if np.all(row == zeros) else True for row in unique3['vector']]]\n",
    "uniq4_notna = unique4[[False if np.all(row == zeros) else True for row in unique4['vector']]]\n",
    "uniq5_notna = unique5[[False if np.all(row == zeros) else True for row in unique5['vector']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #Tokens unique to chapter 1\n",
    "# print('unique:', unique1.shape[0], 'unique notna:', uniq1_notna.shape[0])\n",
    "# print('Total tokens in chapter:', len(wv_df1['token'].values))\n",
    "# print('\\n')\n",
    "# print('unique:', unique2.shape[0], 'unique notna:', uniq2_notna.shape[0])\n",
    "# print('Total tokens in chapter:', len(wv_df2['token'].values))\n",
    "# print('\\n')\n",
    "# print('unique:', unique3.shape[0], 'unique notna:', uniq3_notna.shape[0])\n",
    "# print('Total tokens in chapter:', len(wv_df3['token'].values))\n",
    "# print('\\n')\n",
    "# print('unique:', unique4.shape[0], 'unique notna:', uniq4_notna.shape[0])\n",
    "# print('Total tokens in chapter:', len(wv_df4['token'].values))\n",
    "# print('\\n')\n",
    "# print('unique:', unique5.shape[0], 'unique notna:', uniq5_notna.shape[0])\n",
    "# print('Total tokens in chapter:', len(wv_df5['token'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA for unique vectors, excluding empty vectors\n",
    "unique_notna = uniq1_notna.append([uniq2_notna, uniq3_notna, uniq4_notna, uniq5_notna])\n",
    "unique_notna_pca = do_pca(unique_notna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot PCA of  unique word vectors together. Excludes NA vectors\n",
    "plot_list = [{1: 'red', 2: 'orange', 3: 'yellow', 4: 'green', 5: 'purple'}]\n",
    "xyz = (-4, 2.5, -4, 5, -2, 2.5)\n",
    "plot_pca(unique_notna_pca, plot_list, xyz = xyz, azim = 30, figsize = (12,10), \n",
    "         title = 'PCA of unique word vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot PCA of unique word vectors separately. Excludes NA vectors\n",
    "plot_list = [{1: 'red'}, {2: 'orange'}, {3: 'yellow'}, {4: 'green'}, {5: 'purple'}]\n",
    "\n",
    "plot_pca(unique_notna_pca, plot_list, figsize = (12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazing discovery re: the blob sticking out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What's that blob in the corner?  'PC1' < -2, PC2' > -1, PC3' < -1\n",
    "\n",
    "#For NON normalized unique tokens\n",
    "#The blob is all names of people and names of places\n",
    "xyz = [-10, 1, 1.5, 10, -10, 0]\n",
    "blob_notna = isolate(unique_notna_pca, xyz = xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_notna.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_notna[blob_notna['label'] == 4].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prismXYZ = {'names': [-1, 1, 1.5, 4, -2, -1]}\n",
    "plot_list = [{1: 'red', 2: 'orange', 3: 'yellow', 4: 'green', 5: 'purple'}]\n",
    "xyz = (-4, 2.5, -4, 5, -2, 2.5)\n",
    "plot_pca(unique_notna_pca, plot_list, xyz = xyz, azim = 30, alpha = 0.5, figsize = (12,10), \n",
    "         prismXYZ = prismXYZ, title = 'PCA of unique word vectors, names in cube')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other patterns to draw cubes for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Government, politics, nations, courts\n",
    "#Very few in chapter 1, a lot in chapter 5\n",
    "xyz = [0, 3, -4, 0, -1.5, -1]\n",
    "gov = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prismXYZ = {'gov': [0, 3, -4, 0, -1.5, -1]}\n",
    "plot_list = [{1: 'red', 2: 'orange', 3: 'yellow', 4: 'green', 5: 'purple'}]\n",
    "xyz = (-4, 2.5, -4, 5, -2, 2.5)\n",
    "plot_pca(unique_notna_pca, plot_list, xyz = xyz, azim = 45, alpha = 0.3, figsize = (12,10), \n",
    "         prismXYZ = prismXYZ, title = 'PCA of unique word vectors, gov inscribed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few army words but not as striking as 'gov'\n",
    "xyz = [0, 1, -4, 0, -1, -0.5]\n",
    "part = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sports!!! Flags, winning, games\n",
    "xyz = [-1, 0, -4, 0, -1, -0.5]\n",
    "sports = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prismXYZ = {'sports': [-1, 0, -4, 0, -1, -0.5]}\n",
    "plot_list = [{1: 'red', 2: 'orange', 3: 'yellow', 4: 'green', 5: 'purple'}]\n",
    "xyz = (-4, 2.5, -4, 5, -2, 2.5)\n",
    "plot_pca(unique_notna_pca, plot_list, xyz = xyz, azim = 45, alpha = 0.3, figsize = (12,10), \n",
    "         prismXYZ = prismXYZ, title = 'PCA of unique word vectors, sports!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commercial buildings, waterfront\n",
    "xyz = [-3, -1, -4, 0, -1, -0.5]\n",
    "water = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stuff: nouns, very few verbs\n",
    "xyz = [-2, -1, -4, 0, -0.5, -0]\n",
    "stuff = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDEA: k nearest neighbors?\n",
    "#2667\t-2.376713\t-0.010858\t-0.081842\t4\tsandy\n",
    "#2812\t-2.579536\t-0.781950\t-0.204029\t4\tbeach\n",
    "\n",
    "#A lot of verbs here, present tense not continuous\n",
    "xyz = [0, 0.5, -4, 0, 0, 0.5]\n",
    "verbs = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-ing verbs, but not in chapter 5\n",
    "xyz = [-0.5, 0, -4, 0, 0, 0.5]\n",
    "ing = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Short words?\n",
    "xyz = [-1, -0.5, -4, 0, 0, 0.5]\n",
    "short = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plural nouns\n",
    "xyz = [-3, -1, -4, 0, 0, 0.5]\n",
    "plural = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-ed verbs (past tense)\n",
    "xyz = [-0.5, 2, -4, 0, 0.5, 1]\n",
    "ed = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a lot of short plural nouns and verbs\n",
    "xyz = [-2.5, -0.5, -4, 0, 0.5, 1]\n",
    "moreshort = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fabric and food\n",
    "xyz = [-3, -2.5, -4, 0, 0.5, 1]\n",
    "fabric = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative feelings\n",
    "xyz = [0.5, 2, -4, 0, 1, 1.5]\n",
    "neg = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oceanfaring verbs, ocean weather\n",
    "xyz = [-1.5, 0, -4, 0, 1, 1.5]\n",
    "ocean = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thigns in a house: food, junk, textures\n",
    "xyz = [-3.5, -1.5, -4, 0, 1, 1.5]\n",
    "things = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More negative ing verbs\n",
    "xyz = [-1, 2, -4, 0, 1.5, 2]\n",
    "neging = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textures, descriptions of light\n",
    "xyz = [-2.5, -1, -4, 0, 1.5, 2]\n",
    "light = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twisting, coiling\n",
    "xyz = [-1, 2, -4, 0, 2, 2.5]\n",
    "twist = isolate(unique_notna_pca, xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model: Logistic regression.\n",
    "## Sentence vectors with principal components as features.\n",
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>vector</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[upon, time, good, time, moocow, coming, along...</td>\n",
       "      <td>[0.16427054, 0.12741734, -0.063235335, -0.1478...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[met, nicens, little, boy, named, baby, tuckoo]</td>\n",
       "      <td>[-0.035361428, 0.11802828, -0.28247643, 0.0138...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[father, told, story, father, looked, glass, h...</td>\n",
       "      <td>[-0.1338875, -0.10129124, -0.16580924, 0.22228...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[baby, tuckoo]</td>\n",
       "      <td>[-0.07225, 0.163415, -0.314415, 0.09163, 0.135...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[moocow, came, road, betty, byrne, lived, sold...</td>\n",
       "      <td>[0.07056996, 0.28936455, 0.14651866, -0.130147...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               token  \\\n",
       "0  [upon, time, good, time, moocow, coming, along...   \n",
       "1    [met, nicens, little, boy, named, baby, tuckoo]   \n",
       "2  [father, told, story, father, looked, glass, h...   \n",
       "3                                     [baby, tuckoo]   \n",
       "4  [moocow, came, road, betty, byrne, lived, sold...   \n",
       "\n",
       "                                              vector  label  \n",
       "0  [0.16427054, 0.12741734, -0.063235335, -0.1478...      1  \n",
       "1  [-0.035361428, 0.11802828, -0.28247643, 0.0138...      1  \n",
       "2  [-0.1338875, -0.10129124, -0.16580924, 0.22228...      1  \n",
       "3  [-0.07225, 0.163415, -0.314415, 0.09163, 0.135...      1  \n",
       "4  [0.07056996, 0.28936455, 0.14651866, -0.130147...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine sentence vectors dfs into one\n",
    "feats = ['token', 'vector', 'label']\n",
    "sv_df = sv_df1[feats].append([sv_df2[feats], sv_df3[feats], sv_df4[feats], sv_df5[feats]], \n",
    "                             ignore_index = True)\n",
    "sv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(sv_df['vector'].values)).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(df,\n",
    "         pca_components = 3,\n",
    "         normalize_vector = False,\n",
    "         features_to_norm = None, \n",
    "         other_features = None,\n",
    "         ):\n",
    "    '''\n",
    "    Pipeline for preparing data for modeling.\n",
    "    Parameters:\n",
    "    df (pd.DataFrame) Sentence vector DataFrame.\n",
    "    pca_components (int) Number of principal components to extract from vectors.\n",
    "    features_to_norm (list) Features (other than 'vector') to include in the model.\n",
    "    other_features (list) Features to include without normalizing.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    #1 Turn vector into columns\n",
    "    vector_cols = vec_to_col(df['vector'].values)\n",
    "    \n",
    "    #2 Use PCA to get a few columns from the hundreds of vector columns\n",
    "    pca = PCA(n_components = pca_components)\n",
    "        #Returns an array with as many columns as you chose components\n",
    "    if normalize_vector:\n",
    "        principalComponents = pca.fit_transform(StandardScaler().fit_transform(vector_cols))\n",
    "    else:\n",
    "        principalComponents = pca.fit_transform(vector_cols)   \n",
    "        #Create a new DataFrame for principal components \n",
    "    columns = ['PC' + str(i + 1) for i in range(principalComponents.shape[1])]\n",
    "    pca_df = pd.DataFrame(data = principalComponents, columns = columns)\n",
    "      \n",
    "    #4 Combine features into df\n",
    "    X = copy.deepcopy(pca_df)\n",
    "    \n",
    "        #If features other than the vectors are to be normalized\n",
    "    if features_to_norm:\n",
    "        normed = StandardScaler().fit_transform(df[features_to_norm])\n",
    "        normed_df = pd.DataFrame(data = normed, columns = features_to_norm)\n",
    "        X = pd.concat([X, normed_df], axis = 1)\n",
    "    \n",
    "        #If any other features, not normalized, are to be included\n",
    "    if other_features:\n",
    "        X = pd.concat([X, df[other_features]], axis = 1)\n",
    "    \n",
    "    #5 Get target\n",
    "    y = df['label']\n",
    "    \n",
    "    #[OPTIONAL] Adjust sizes of chapters  ####\n",
    "    \n",
    "    #6 Do train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12)\n",
    "    \n",
    "    ##SPLIT OFF LOG REG INTO SEPARATE PIPELINE FUNCTION?\n",
    "    \n",
    "    #7 Instantiate model\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e16)\n",
    "    \n",
    "    #8 Fit the training data to the model\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    #9 Generate predicted values for y to compare to real values\n",
    "    y_hat_train = logreg.predict(X_train)\n",
    "    #Now generate predicted values for the test data to compare\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "    \n",
    "    #Print scores\n",
    "    target_names = ['1', '2', '3', '4', '5']\n",
    "    print('LogReg, training set:')\n",
    "    print(classification_report(y_train, y_hat_train, target_names = target_names))\n",
    "    print('LogReg, testing set:')\n",
    "    print(classification_report(y_test, y_hat_test, target_names = target_names))\n",
    "    \n",
    "    return y_hat_train, y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>vector</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[upon, time, good, time, moocow, coming, along...</td>\n",
       "      <td>[0.16427054, 0.12741734, -0.063235335, -0.1478...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[met, nicens, little, boy, named, baby, tuckoo]</td>\n",
       "      <td>[-0.035361428, 0.11802828, -0.28247643, 0.0138...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[father, told, story, father, looked, glass, h...</td>\n",
       "      <td>[-0.1338875, -0.10129124, -0.16580924, 0.22228...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[baby, tuckoo]</td>\n",
       "      <td>[-0.07225, 0.163415, -0.314415, 0.09163, 0.135...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[moocow, came, road, betty, byrne, lived, sold...</td>\n",
       "      <td>[0.07056996, 0.28936455, 0.14651866, -0.130147...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               token  \\\n",
       "0  [upon, time, good, time, moocow, coming, along...   \n",
       "1    [met, nicens, little, boy, named, baby, tuckoo]   \n",
       "2  [father, told, story, father, looked, glass, h...   \n",
       "3                                     [baby, tuckoo]   \n",
       "4  [moocow, came, road, betty, byrne, lived, sold...   \n",
       "\n",
       "                                              vector  label  \n",
       "0  [0.16427054, 0.12741734, -0.063235335, -0.1478...      1  \n",
       "1  [-0.035361428, 0.11802828, -0.28247643, 0.0138...      1  \n",
       "2  [-0.1338875, -0.10129124, -0.16580924, 0.22228...      1  \n",
       "3  [-0.07225, 0.163415, -0.314415, 0.09163, 0.135...      1  \n",
       "4  [0.07056996, 0.28936455, 0.14651866, -0.130147...      1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg, training set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.08      0.14      1090\n",
      "           2       0.00      0.00      0.00       643\n",
      "           3       0.46      0.23      0.31       742\n",
      "           4       0.00      0.00      0.00       310\n",
      "           5       0.39      0.99      0.56      1522\n",
      "\n",
      "   micro avg       0.41      0.41      0.41      4307\n",
      "   macro avg       0.37      0.26      0.20      4307\n",
      "weighted avg       0.47      0.41      0.29      4307\n",
      "\n",
      "LogReg, testing set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.07      0.13       364\n",
      "           2       0.00      0.00      0.00       213\n",
      "           3       0.45      0.23      0.30       216\n",
      "           4       0.00      0.00      0.00       110\n",
      "           5       0.41      1.00      0.58       533\n",
      "\n",
      "   micro avg       0.42      0.42      0.42      1436\n",
      "   macro avg       0.37      0.26      0.20      1436\n",
      "weighted avg       0.47      0.42      0.29      1436\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ssrosa/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Logistic regressio model\n",
    "y_hat_train, y_hat_test = pipe(sv_df, \n",
    "                      normalize_vector = True,\n",
    "                         features_to_norm = None,\n",
    "                      other_features = ['label'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESUME HERE.\n",
    "\n",
    "## Pipeline is working! First model doesn't perform well. Can try different amounts of PCA components, and can also try downsampling from chapter 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop(word):\n",
    "    '''\n",
    "    Parameters:\n",
    "    word (str)\n",
    "    '''\n",
    "    stop_list = stopwords.words('english')\n",
    "    stop_list += list(string.punctuation)\n",
    "    stop_list += ['could', 'like', 'one', 'said', 'would', '\\'s', '\\'ll']\n",
    "    \n",
    "    if word.isalpha():\n",
    "        if word.lower() not in stop_list:\n",
    "            return word.lower()  \n",
    "\n",
    "# FOR CONSTRUCTING DFS OF VECTORS ###########################\n",
    "def word_vec_df(chapter, label):\n",
    "    '''\n",
    "    Constructs a df to track words in a chapter, each word's token,\n",
    "    and each token's vector.\n",
    "    Parameters:\n",
    "    chapter (spacy.tokens.doc.Doc) The text of a chapter.\n",
    "    label (int) The chapter number.\n",
    "    Returns:\n",
    "    df (pd.DataFrame)\n",
    "    '''\n",
    "   \n",
    "    #Token for each word in the text that survives stoppage  \n",
    "    #Only need unique tokens, so we take the set\n",
    "    tokens = list(set([stop(t) for t in word_tokenize(chapter.text) if stop(t)]))\n",
    "    \n",
    "    #Vector for each token\n",
    "    vectors = [nlp.vocab[t].vector for t in tokens]\n",
    "    \n",
    "    #Instantiate new df\n",
    "    df = pd.DataFrame()\n",
    "    #Build df columns from above lists\n",
    "    df['token'] = tokens\n",
    "    df['vector'] = vectors\n",
    "    df['label'] = label\n",
    "    \n",
    "    return df\n",
    "\n",
    "def sent_vec_df(chapter, labels):\n",
    "    '''\n",
    "    Consructs a df to track a chapter's sentences, each sentence's tokens,\n",
    "    some stats on the tokens, each token's vector, and the \n",
    "    'mean vector' for each sentence's vectors.\n",
    "   \n",
    "    Parameters:\n",
    "    chapter (spacy.tokens.doc.Doc) The text of a chapter.\n",
    "    label (int) The chapter number.\n",
    "    \n",
    "    Returns:\n",
    "    df (pd.DataFrame)\n",
    "    '''\n",
    "    #Turn spacy doc into lists of sentences as strings\n",
    "    sentences = [str(s) for s in list(chapter.sents)]\n",
    "    \n",
    "    #Number of characters in each sentence\n",
    "    char_counts = [len(s) for s in sentences]\n",
    "   \n",
    "    #The tokens for each sentence\n",
    "    tokenized_sentences = [[stop(t) for t in word_tokenize(sentence) if stop(t)] \\\n",
    "                               for sentence in sentences]\n",
    "    \n",
    "    #Average length of token in each sentence\n",
    "    mean_token_len = [np.mean([len(t) for t in tokens]\n",
    "                             ) for tokens in tokenized_sentences]\n",
    "    #Number of tokens in each sentence\n",
    "    #(Those with 0 will be dropped later)\n",
    "    token_counts = [len(t) for t in tokenized_sentences]\n",
    "                               \n",
    "    #The vector for each token\n",
    "    vectorized_tokens = [[nlp.vocab[t].vector for t in tokenized_sent] \\\n",
    "                             for tokenized_sent in tokenized_sentences]\n",
    "    \n",
    "    #The 'mean vector' for each sentence's vectors\n",
    "    #(Each mean vector is itself a 300-dimension vector like the originals)\n",
    "    mean_vectors = [np.mean([v for v in vector_list], \n",
    "                           axis = 0\n",
    "                          ) for vector_list in vectorized_tokens]\n",
    "    \n",
    "    #Instantiate new df\n",
    "    df = pd.DataFrame()\n",
    "    #Build df columns from above lists\n",
    "    df['sentence'] = sentences\n",
    "    df['char_count'] = char_counts\n",
    "    df['token'] = tokenized_sentences\n",
    "    df['mean_token_len'] = mean_token_len\n",
    "    df['token_count'] = token_counts\n",
    "    df['vectors'] = vectorized_tokens\n",
    "    df['vector'] = mean_vectors\n",
    "    df['label'] = labels\n",
    "    \n",
    "    #Drop rows which don't have any tokens\n",
    "    df = df[df['token_count'] != 0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def appears_in(text, chap, label):\n",
    "    '''\n",
    "    Gets words that are unique to a chapter. Checks a chapter's words\n",
    "    against the words in the rest of the text.\n",
    "   \n",
    "    Parameters:\n",
    "    text (pd.DataFrame) The full text. (Requires either a sv_df or wv_df.)\n",
    "    chap (pd.DataFrame) Just this chapter.\n",
    "    label (int) This chapter's number.\n",
    "    \n",
    "    Return:\n",
    "    df (pd.DataFrame)\n",
    "    '''\n",
    "    #List of words from other chapters\n",
    "    other_chapters = text[text['label'] != label]['token'].values\n",
    "    \n",
    "    #List of words from this chapter\n",
    "    this_chapter = chap['token'].values\n",
    "    \n",
    "    #List of bools for words unique to this chapter\n",
    "    unique_words = [True if v not in other_chapters else False \\\n",
    "                     for v in this_chapter]\n",
    "    df = chap[unique_words]\n",
    "    return df\n",
    "\n",
    "# PCA functions ##############################################################\n",
    "#vec_to_col\n",
    "def vec_to_col(vector_arr):\n",
    "    '''\n",
    "    Turns an array of vectors into columns for each vector.\n",
    "    Use df['vector'].values to pass in the array of vectors.\n",
    "    It gets turned into a 300-wide array with as many rows as rows in the df.\n",
    "    '''\n",
    "    #Make new 2d array\n",
    "    data = np.array(list(vector_arr))\n",
    "    \n",
    "    #Make list of column names\n",
    "    #Assumes all vectors have same dimensions\n",
    "    columns = [str(i) for i in range(data.shape[1])]\n",
    "    \n",
    "    #Instantiate new df to hold vector dimensions as columns\n",
    "    vector_cols = pd.DataFrame(data = data, columns = columns)\n",
    "\n",
    "    return vector_cols\n",
    "#def get_x_y(df):\n",
    "    \n",
    "\n",
    "def do_pca(df, veclen = 300, normalize = False, components = 3):    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #Turn vectors into columns.\n",
    "    vector_cols = vec_to_col(df['vector'].values)\n",
    "    \n",
    "    #Attach vector columns to dataframe\n",
    "    df = pd.concat([df, vector_cols], axis = 1)\n",
    "    \n",
    "    #Get features and target.\n",
    "    features = df.drop(['token', 'vector', 'label'], axis = 1, inplace = False)\n",
    "    target = df['label']\n",
    "    \n",
    "    if normalize:\n",
    "        X = StandardScaler().fit_transform(features)\n",
    "    else:\n",
    "        X = features\n",
    "        \n",
    "    #Get principal components from features with PCA object\n",
    "    pca = PCA(n_components = components)\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "\n",
    "    # Create a new DataFrame for principal components \n",
    "    columns = ['PC' + str(i + 1) for i in range(principalComponents.shape[1])]\n",
    "    pca_df = pd.DataFrame(data = principalComponents, columns = columns)\n",
    "   \n",
    "    #Add other features back\n",
    "    pca_df['label'] = target.values\n",
    "    pca_df['token'] = df['token'].values\n",
    "    pca_df['vector'] = df['vector'].values\n",
    "    \n",
    "    return pca_df\n",
    "\n",
    "def get_xyz(xyz):\n",
    "    '''\n",
    "    Helper function for plotting.\n",
    "    Parameters:\n",
    "    xyz (list) List with 6 integers.\n",
    "    Returns:\n",
    "    6 integers in variables\n",
    "    '''\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = xyz[0], xyz[1], xyz[2], xyz[3], xyz[4], xyz[5]\n",
    "    return xmin, xmax, ymin, ymax, zmin, zmax\n",
    "\n",
    "def plot_pca(df, plot_list, \n",
    "             xyz = None,\n",
    "             figsize = (12, 14),\n",
    "             azim = 60, \n",
    "             elev = 30,\n",
    "             alpha = None,\n",
    "             prismXYZ = None,\n",
    "             legend = False,\n",
    "             title = None):\n",
    "    '''\n",
    "    Draws a flexible figure of subplots, with as many as are listed\n",
    "    in the plot list of dictionaries. Each subplot can have one or more\n",
    "    scatters drawn together depending on how many key/val pairs are in\n",
    "    the dict.\n",
    "    Parameters:\n",
    "    df (pd.DataFrame) Needs columns for 'PC1', 'PC2', and 'PC3.'\n",
    "    plot_list (list) List of dictionaries. Each dict is a subplot with as many \n",
    "    scatters as there are key/val pairs in the dict. E.g. for one plot with one scatter,\n",
    "    use a list with 1 dict with 1 key/val pair.\n",
    "    xyz (list) List of limits for the three dimensions.\n",
    "    figsize (tuple) Size of figure.\n",
    "    azim (int): Horizontal view of the plot.\n",
    "    elev (int): Vertical view of the plot.\n",
    "    alpha (float): Value between 0 and 1. Transparency of scatters.\n",
    "    prismXYZ (dict): Dict with prism names as keys and dimension lists as values.\n",
    "    legend (bool): True if a legend should be drawn.\n",
    "    title (str): Title of plot (only really if just one plot being shown.)\n",
    "    \n",
    "    Returns: \n",
    "    Draws a plot.\n",
    "    '''\n",
    "    #Get xyz limits for the plots.\n",
    "    if xyz:\n",
    "        xmin, xmax, ymin, ymax, zmin, zmax = get_xyz(xyz)\n",
    "    \n",
    "    #Get right number of rows whether length is even or odd.\n",
    "    #Needed this instead of just rounding because of 'banker's rounding.'\n",
    "    rows = (len(plot_list) // 2) + (len(plot_list) % 2)\n",
    "\n",
    "    #Set columns. 1 if 1 plot, else 2.\n",
    "    if len(plot_list) == 1:\n",
    "        cols = 1\n",
    "    else:\n",
    "        cols = 2\n",
    "    #Draw figure to hold all plots\n",
    "    fig = plt.figure(figsize = figsize)    \n",
    "    \n",
    "    #Draw a subplot for each dict in the plot list.\n",
    "    #If just one subplot should be drawn, put just one dict in the list.\n",
    "    #The plot will fill the figure.\n",
    "    for i in range(len(plot_list)): \n",
    "        ax = fig.add_subplot(rows, cols, (i + 1), \n",
    "                             projection='3d', \n",
    "                             azim = azim, \n",
    "                             elev = elev)\n",
    "        if xyz: \n",
    "            plt.xlim(xmin, xmax)\n",
    "            plt.ylim(ymin, ymax)\n",
    "            ax.set_zlim(zmin, zmax)\n",
    "        \n",
    "        #Draw each scatter within a subplot. 1 for each key/val pair in the dict.\n",
    "        #(Allows for each subplot to show one or more scatters.)\n",
    "        #together.\n",
    "        for label, color in  plot_list[i].items():\n",
    "            #Pull the data from the df corresponding to each label\n",
    "            pca = df[df['label'] == label]\n",
    "            #Draw a scatter for the data from each label\n",
    "            ax.scatter(xs = pca['PC1'], \n",
    "                       ys = pca['PC2'], \n",
    "                       zs = pca['PC3'],\n",
    "                       c = color,\n",
    "                       alpha = alpha,\n",
    "                       label = ('Chapter ' + str(label)))\n",
    "        \n",
    "        #Draw prisms\n",
    "        if prismXYZ:\n",
    "            #Draw each prism in list\n",
    "            #Leaving prismXYZ as a dict for the moment in case I want to use labels\n",
    "            for label, xyz in prismXYZ.items():\n",
    "                draw_prism(xyz, 'black')\n",
    "        \n",
    "            \n",
    "        #Label axes for each subplot\n",
    "        ax.set_xlabel('x, PC1')\n",
    "        ax.set_ylabel('y, PC2')\n",
    "        ax.set_zlabel('z, PC3')\n",
    "        \n",
    "        #Set legend for each subplot\n",
    "        if legend:\n",
    "            plt.legend()\n",
    "        #Set title for each subplot\n",
    "        if not title:\n",
    "            plt.title('Chapters: ' + str(list(plot_list[i].keys())))\n",
    "        else:\n",
    "            plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def isolate(df, xyz):\n",
    "    '''\n",
    "    Parameters:\n",
    "    df (pd.DataFrame) Needs a 'label' column to work. Needs PC1, 2, and 3 columns.\n",
    "    xyz (list) List of limits for PC1, 2, and 3.\n",
    "   \n",
    "    Returns:\n",
    "    blob_df (pd.DataFrame) Section of the pca_df within the \n",
    "    spatial limits set by the parameters. The 'blob' within \n",
    "    a 'cube' on a 3d plot.\n",
    "    '''\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = get_xyz(xyz)\n",
    "    \n",
    "    blob = df[\n",
    "                    ((df['PC1'] > xmin) & (df['PC1'] < xmax )) & \\\n",
    "                    ((df['PC2'] > ymin) & (df['PC2'] < ymax )) & \\\n",
    "                    ((df['PC3'] > zmin) & (df['PC3'] < zmax ))\n",
    "                ]\n",
    "    return blob\n",
    "\n",
    "def draw_prism(xyz, color):\n",
    "    '''\n",
    "    When called within plot_pca, draws a prism in a 3d plot.\n",
    "    Works well with isolate() to visualize the isolated tokens.\n",
    "    '''\n",
    "    #1 .   2 .   3 .   4 .   5 .  6\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax = get_xyz(xyz)\n",
    "    c = color\n",
    "    plt.plot([], [], [], color = c)\n",
    "    \n",
    "    kwargs = {'linewidth': 4}\n",
    "    \n",
    "    #I tried a lot of ways of consolidating these loops further\n",
    "    #but decided to move on.\n",
    "    #x limits at zmin\n",
    "    for x in [xmin, xmax]:\n",
    "        plt.plot([x, x], [ymin, ymax], [zmin, zmin], color = c, **kwargs)\n",
    "\n",
    "    #y limiits at zmin\n",
    "    for y in [ymin, ymax]:\n",
    "        plt.plot([xmin, xmax], [y, y], [zmin, zmin], color = c, **kwargs)\n",
    "\n",
    "    #x limits at zmax\n",
    "    for x in [xmin, xmax]:\n",
    "        plt.plot([x, x,], [ymin, ymax,], [zmax, zmax], color = c, **kwargs)\n",
    "\n",
    "    #y limits at zmax\n",
    "    for y in [ymin, ymax]:\n",
    "        plt.plot([xmin, xmax], [y, y], [zmax, zmax], color = c, **kwargs)\n",
    "\n",
    "    #z limits\n",
    "    for x in [xmin, xmax]:\n",
    "        for y in [ymin, ymax]:\n",
    "            plt.plot([x, x], [y, y], [zmin, zmax], color = c, **kwargs)\n",
    "            \n",
    "    #Draw label\n",
    "    #ax.text(xmax, ymax, zmin, s = label)\n",
    "##############################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST FOR MULTIPLE CUBES IN ONE PLOT\n",
    "\n",
    "# #twisting\n",
    "# names = [-1, 1, 1.5, 4, -2, -1]\n",
    "# twisting = [-1, 2, -4, 0, 2, 2.5]\n",
    "# #Sports!!! Flags, winning, games\n",
    "# sports = [-1, 0, -4, 0, -1, -0.5]\n",
    "# prismXYZ = {'names': names, \n",
    "#             'twisting': twisting, \n",
    "#             'sports': sports}\n",
    "# #TEST PRISM for twisting curling\n",
    "# plot_list = [{1: 'red', 2: 'orange', 3: 'yellow', 4: 'green', 5: 'purple'}]\n",
    "# xyz = (-4, 2.5, -4, 5, -2, 2.5)\n",
    "# plot_pca(unique_notna_pca, plot_list, xyz = xyz, azim = 30, alpha = 0.7, figsize = (12,10),\n",
    "#          prismXYZ = prismXYZ,\n",
    "#          title = 'PCA of unique word vectors with prism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions from Parrish\n",
    "\n",
    "# def subtractv(coord1, coord2):\n",
    "#     return [c1 - c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "# def addv(coord1, coord2):\n",
    "#     return [c1 + c2 for c1, c2 in zip(coord1, coord2)]\n",
    "\n",
    "# def meanv(coords):\n",
    "#     # assumes every item in coords has same length as item 0\n",
    "#     sumv = [0] * len(coords[0])\n",
    "#     for item in coords:\n",
    "#         for i in range(len(item)):\n",
    "#             sumv[i] += item[i]\n",
    "#     mean = [0] * len(sumv)\n",
    "#     for i in range(len(sumv)):\n",
    "#         mean[i] = float(sumv[i]) / len(coords)\n",
    "#     return mean\n",
    "\n",
    "\n",
    "# #Convenience function\n",
    "# def vec(s):\n",
    "#     return nlp.vocab[s].vector\n",
    "\n",
    "# # cosine similarity\n",
    "# def cosine(v1, v2):\n",
    "#     if norm(v1) > 0 and norm(v2) > 0:\n",
    "#         return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "#     else:\n",
    "#         return 0.0\n",
    "    \n",
    "# def spacy_closest(token_list, vec_to_check, n=10):\n",
    "#     return sorted(token_list,\n",
    "#                   key=lambda x: cosine(vec_to_check, vec(x)),\n",
    "#                   reverse=True)[:n]\n",
    "\n",
    "# def sentvec(s):\n",
    "#     sent = nlp(s)\n",
    "#     return meanv([w.vector for w in sent])\n",
    "\n",
    "# def spacy_closest_sent(space, input_str, n=10):\n",
    "#     input_vec = sentvec(input_str)\n",
    "#     return sorted(space,\n",
    "#                   key=lambda x: cosine(np.mean([w.vector for w in x], axis=0), input_vec),\n",
    "#                   reverse=True)[:n]\n",
    "\n",
    "#Tinkering from Parrish:\n",
    "\n",
    "# cosine(vec('dog'), vec('puppy')) > cosine(vec('trousers'), vec('pants'))\n",
    "# spacy_closest(tokens5, vec(\"pie\"))\n",
    "# blue_to_sky = subtractv(vec(\"blue\"), vec(\"sky\"))\n",
    "# spacy_closest(tokens5, addv(blue_to_sky, vec(\"grass\")))\n",
    "\n",
    "#spacy_closest(ch1_nlp_tokens, vec(ch1_nlp_tokens[0]))\n",
    "\n",
    "#sents1 = list(ch3.sents)\n",
    "#for sent in spacy_closest_sent(sents1, \"I went to church this morning.\"):\n",
    "#    print(sent.text)\n",
    "#    print(\"---\")\n",
    "\n",
    "#From Parrish\n",
    "#nlp.vocab['cheese'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight workflow\n",
    "### Data cleaning:\n",
    "- tokenize\n",
    "- remove non-alphanumeric\n",
    "- to lower case\n",
    "- combine words with similar spellings\n",
    "- lemmatize?\n",
    "\n",
    "### Words into numbers\n",
    "- bag words [0 0 0 0 0 1 0 0 ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
